{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f89a37",
   "metadata": {},
   "source": [
    "# Chest X‑ray Classification with a Vision Transformer (ViT)\n",
    "\n",
    "**Task:** Binary classification (Normal vs Pneumonia) on the public Kaggle dataset:  \n",
    "<https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia>\n",
    "\n",
    "**Key requirements satisfied:**  \n",
    "- Leave the **test** set **completely untouched** for training/tuning.  \n",
    "- You may adjust the train/validation split; this notebook provides two options:\n",
    "  1) Use the dataset-provided `val/` split as-is, or  \n",
    "  2) Rebuild validation from `train/` (optionally also include the small original `val/`) **without** touching `test/`.\n",
    "\n",
    "**Model:** A from-scratch **Vision Transformer (ViT)** implemented in PyTorch, adhering to the paper’s core design:  \n",
    "- Split the image into non-overlapping patches and linearly project each patch to a latent dimension (Eq. 1).  \n",
    "- Prepend a **learnable [CLS] token** whose final hidden state is used for classification (Eq. 4).  \n",
    "- Add **learnable 1‑D positional embeddings** to tokens.  \n",
    "- Use a **Transformer encoder** stack with pre-LayerNorm, **Multi-Head Self-Attention** (MSA) and an **MLP (GELU)** block with residual connections (Eqs. 2–3).  \n",
    "- Linear classification head on top of the final [CLS] representation.\n",
    "\n",
    "> The design follows **“An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale” (ViT)**, ICLR 2021.  \n",
    "> Dosovitskiy et al., 2021. (See discussion in comments inside code.)\n",
    "\n",
    "---\n",
    "\n",
    "### What you’ll get\n",
    "- Data loader for the Kaggle folder layout (`train/`, `val/`, `test/`) and an option to rebuild validation from `train/` without touching `test/`.\n",
    "- From-scratch **ViT** (patchify via `Conv2d` with stride=`patch_size`, class token, learned positional embeddings, pre-LN encoder blocks).\n",
    "- **Training loop** with:\n",
    "  - **AdamW** optimizer + cosine LR schedule with warmup\n",
    "  - **AMP** (mixed precision) if CUDA available\n",
    "  - **Weighted sampler** or **pos_weight** to mitigate class imbalance\n",
    "  - **Early stopping** on validation AUROC (default) or F1\n",
    "- **Final evaluation on the untouched test set** (Accuracy, Precision, Recall, F1, AUROC, confusion matrix, ROC curve).\n",
    "- Optional: if `timm`/`torchvision` pretrained ViT is available locally, you can switch to fine-tuning with a single flag.\n",
    "\n",
    "> **Note:** Chest X‑rays are grayscale; we replicate to 3 channels for ViT. You can also adapt the patch embedding to 1 channel (a switch is provided).\n",
    "\n",
    "---\n",
    "\n",
    "### How to use\n",
    "1. Run the **Setup & Data** section. If the dataset folder isn’t present, the notebook can try to download via Kaggle CLI (you must provide a valid `kaggle.json`).\n",
    "2. Configure the **Training Config** cell (image size, patch size, depth, heads, etc.).\n",
    "3. Run **Train**.  \n",
    "4. The notebook automatically **saves the best model** (by validation metric) and then runs a **one-time test evaluation**.\n",
    "\n",
    "---\n",
    "\n",
    "*Attribution / Reference:* The ViT architecture elements (patch tokenization, [CLS] token, 1‑D positional embeddings, pre-LN MSA+MLP blocks, and fine‑tuning guidance) are based on the ICLR 2021 paper by Dosovitskiy et al., *An Image is Worth 16×16 Words*. See Eqs. (1)–(4), Sec. 3.1–3.2 for the core design and fine‑tuning notes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c109bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Setup: Imports & Environment ====\n",
    "import os, sys, math, time, random, shutil, subprocess\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset, WeightedRandomSampler\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "try:\n",
    "    import timm  # optional: for pretrained ViT fine-tuning if available locally\n",
    "    _HAVE_TIMM = True\n",
    "except Exception:\n",
    "    _HAVE_TIMM = False\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_fscore_support\n",
    "    _HAVE_SKLEARN = True\n",
    "except Exception as e:\n",
    "    _HAVE_SKLEARN = False\n",
    "    print(\"scikit-learn not found; installing may improve metric reports.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826871cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Paths & Kaggle Download Helper ====\n",
    "from pathlib import Path\n",
    "import os, shutil, subprocess\n",
    "\n",
    "# Working directory note: If you are in notebooks/, this recreates the path the prompt expects.\n",
    "DATASET_DIR = Path.cwd().parent / \"Data\" / \"chest_xray\"\n",
    "KAGGLE_ZIP = DATASET_DIR.parent / \"chest-xray-pneumonia.zip\"\n",
    "\n",
    "def has_dataset_structure(root: Path) -> bool:\n",
    "    return (root / \"train\").exists() and (root / \"val\").exists() and (root / \"test\").exists()\n",
    "\n",
    "def try_kaggle_download():\n",
    "    kaggle_json_candidates = [Path(\"./kaggle.json\"), Path(\"/content/kaggle.json\")]\n",
    "    for cand in kaggle_json_candidates:\n",
    "        if cand.exists():\n",
    "            os.makedirs(Path.home() / \".kaggle\", exist_ok=True)\n",
    "            shutil.copy(str(cand), str(Path.home() / \".kaggle/kaggle.json\"))\n",
    "            os.chmod(Path.home() / \".kaggle/kaggle.json\", 0o600)\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        print(\"Attempting Kaggle download...\")\n",
    "        DATASET_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
    "        subprocess.run(\n",
    "            [\"kaggle\", \"datasets\", \"download\", \"-d\", \"paultimothymooney/chest-xray-pneumonia\", \"-p\", str(DATASET_DIR.parent)],\n",
    "            check=True\n",
    "        )\n",
    "        if KAGGLE_ZIP.exists():\n",
    "            print(\"Extracting zip...\")\n",
    "            shutil.unpack_archive(str(KAGGLE_ZIP), str(DATASET_DIR.parent))\n",
    "        else:\n",
    "            print(\"Download finished but zip not found at\", KAGGLE_ZIP)\n",
    "    except Exception as e:\n",
    "        print(\"Kaggle download failed:\", e)\n",
    "\n",
    "if not has_dataset_structure(DATASET_DIR):\n",
    "    print(\"Dataset folder not found at\", DATASET_DIR.resolve())\n",
    "    if shutil.which(\"kaggle\") is None:\n",
    "        print(\"Kaggle CLI not found. Install with `pip install kaggle` and rerun, or upload chest_xray/ manually.\")\n",
    "    else:\n",
    "        try_kaggle_download()\n",
    "\n",
    "print(\"Dataset present?\", has_dataset_structure(DATASET_DIR), \"->\", DATASET_DIR.resolve())\n",
    "if not has_dataset_structure(DATASET_DIR):\n",
    "    print(\"❗ Please upload the `chest_xray/` folder (with train/val/test).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed2c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Experiment Configuration ====\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data\n",
    "    dataset_dir: Path = DATASET_DIR\n",
    "    use_original_val_split: bool = False  # If False, rebuild val from train (test remains untouched)\n",
    "    include_small_original_val_into_train: bool = True  # If True and rebuilding val, include the small 'val/' set into the train pool before splitting.\n",
    "    val_fraction_from_train: float = 0.1  # fraction for validation when rebuilding from train\n",
    "    num_workers: int = 4\n",
    "    batch_size: int = 32\n",
    "    # Image & ViT\n",
    "    image_size: int = 224  # training resolution; keep fixed to avoid pos-embed interpolation complexity\n",
    "    in_channels: int = 3   # ViT expects 3; we'll replicate grayscale to 3. If you set to 1, patch embed will adapt.\n",
    "    patch_size: int = 16   # typical ViT configuration (e.g., /16)\n",
    "    embed_dim: int = 256   # smaller than ViT-B to keep training light\n",
    "    depth: int = 6         # number of Transformer blocks\n",
    "    num_heads: int = 8\n",
    "    mlp_ratio: float = 4.0\n",
    "    attn_dropout: float = 0.0\n",
    "    drop_rate: float = 0.1\n",
    "    # Optimization\n",
    "    epochs: int = 15\n",
    "    warmup_epochs: int = 2\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.05\n",
    "    label_smoothing: float = 0.0\n",
    "    # Class imbalance handling\n",
    "    use_weighted_sampler: bool = True\n",
    "    # Early stopping\n",
    "    early_stop_metric: str = \"auroc\"  # one of: \"auroc\", \"f1\"\n",
    "    early_stop_patience: int = 5\n",
    "    # Optional: use local pretrained ViT if available (timm or torchvision). If not present, falls back to scratch ViT.\n",
    "    try_pretrained_vit: bool = False\n",
    "    # Repro\n",
    "    seed: int = 42\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80208c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Datasets & DataLoaders ====\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "def build_transforms(image_size: int, replicate_grayscale_to_3: bool = True):\n",
    "    common = []\n",
    "    if replicate_grayscale_to_3:\n",
    "        common.append(transforms.Grayscale(num_output_channels=3))\n",
    "    else:\n",
    "        common.append(transforms.Grayscale(num_output_channels=1))\n",
    "\n",
    "    train_tfms = transforms.Compose([\n",
    "        *common,\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0), ratio=(1.0, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize using ImageNet stats is fine even from scratch; adjust if desired.\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)) if replicate_grayscale_to_3\n",
    "            else transforms.Normalize(mean=(0.5,), std=(0.25,)),\n",
    "    ])\n",
    "    eval_tfms = transforms.Compose([\n",
    "        *common,\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)) if replicate_grayscale_to_3\n",
    "            else transforms.Normalize(mean=(0.5,), std=(0.25,)),\n",
    "    ])\n",
    "    return train_tfms, eval_tfms\n",
    "\n",
    "train_tfms, eval_tfms = build_transforms(cfg.image_size, replicate_grayscale_to_3=(cfg.in_channels==3))\n",
    "\n",
    "def build_datasets_and_loaders(cfg: Config):\n",
    "    root = cfg.dataset_dir\n",
    "    if not (root / \"train\").exists():\n",
    "        raise FileNotFoundError(f\"Dataset directory not found: {root}\")\n",
    "\n",
    "    # ImageFolder expects a class-per-subfolder structure\n",
    "    ds_train = datasets.ImageFolder(root / \"train\", transform=train_tfms)\n",
    "    ds_val_orig = datasets.ImageFolder(root / \"val\", transform=eval_tfms)\n",
    "    ds_test = datasets.ImageFolder(root / \"test\", transform=eval_tfms)\n",
    "\n",
    "    class_to_idx = ds_train.class_to_idx\n",
    "    idx_to_class = {v:k for k,v in class_to_idx.items()}\n",
    "    print(\"Classes:\", idx_to_class)\n",
    "\n",
    "    if cfg.use_original_val_split:\n",
    "        train_set = ds_train\n",
    "        val_set = ds_val_orig\n",
    "        full_pool = None\n",
    "    else:\n",
    "        # Rebuild validation from training (optionally include original val into the pool)\n",
    "        pool_datasets = [ds_train]\n",
    "        if cfg.include_small_original_val_into_train:\n",
    "            pool_datasets.append(datasets.ImageFolder(root / \"val\", transform=train_tfms))\n",
    "        full_pool = ConcatDataset(pool_datasets)\n",
    "\n",
    "        n_total = len(full_pool)\n",
    "        n_val = int(cfg.val_fraction_from_train * n_total)\n",
    "        n_train = n_total - n_val\n",
    "        train_set, val_set = random_split(full_pool, [n_train, n_val],\n",
    "                                          generator=torch.Generator().manual_seed(cfg.seed))\n",
    "\n",
    "        print(f\"Rebuilt val from train pool: n_train={n_train}, n_val={n_val} (total={n_total})\")\n",
    "\n",
    "    # Weighted sampler for imbalance (optional)\n",
    "    sampler = None\n",
    "    if cfg.use_weighted_sampler:\n",
    "        # Compute class distribution on the training subset\n",
    "        if isinstance(train_set, torch.utils.data.Subset):\n",
    "            # Need to recover targets for the subset\n",
    "            targets = []\n",
    "            assert full_pool is not None, \"Internal error: full_pool should not be None when using rebuilt split.\"\n",
    "            for i in train_set.indices:\n",
    "                # Map global index to sub-dataset\n",
    "                running = 0\n",
    "                for ds in full_pool.datasets:\n",
    "                    if i < running + len(ds):\n",
    "                        targets.append(ds.samples[i - running][1])\n",
    "                        break\n",
    "                    running += len(ds)\n",
    "        else:\n",
    "            targets = [s[1] for s in train_set.samples] if hasattr(train_set, \"samples\") else [y for _, y in train_set]\n",
    "\n",
    "        class_sample_count = np.bincount(np.array(targets), minlength=len(class_to_idx))\n",
    "        class_weights = 1.0 / (class_sample_count + 1e-6)\n",
    "        sample_weights = np.array([class_weights[t] for t in targets], dtype=np.float32)\n",
    "        sampler = WeightedRandomSampler(weights=torch.from_numpy(sample_weights),\n",
    "                                        num_samples=len(sample_weights), replacement=True)\n",
    "        print(\"Class counts:\", class_sample_count, \"-> Using WeightedRandomSampler\")\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=(sampler is None),\n",
    "                              sampler=sampler, num_workers=cfg.num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                            num_workers=cfg.num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(ds_test, batch_size=cfg.batch_size, shuffle=False,\n",
    "                             num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, idx_to_class\n",
    "\n",
    "train_loader, val_loader, test_loader, idx_to_class = build_datasets_and_loaders(cfg)\n",
    "num_classes = len(idx_to_class)\n",
    "assert num_classes == 2, f\"Expected 2 classes, got {num_classes} -> {idx_to_class}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd12219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Vision Transformer (from scratch) ====\n",
    "# Core ideas: patchify (Eq.1), [CLS] token (Eq.4), 1-D positional embeddings, pre-LN MSA + MLP (Eqs. 2–3).\n",
    "# This matches the minimalist ViT design (no convs beyond patchify; no 2-D pos enc; pre-LN; GELU MLP).\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    '''\n",
    "    Splits image into non-overlapping patches, projects each to embed_dim.\n",
    "    Implemented as a Conv2d with kernel_size=stride=patch_size.\n",
    "    Input: (B, C, H, W) -> Output: (B, N, D) where N = (H/P)*(W/P).\n",
    "    '''\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size // patch_size, img_size // patch_size)\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W) -> (B, D, H/P, W/P) -> (B, D, N) -> (B, N, D)\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.0):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x)  # (B, N, 3*C)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)  # (3, B, heads, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each: (B, heads, N, head_dim)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, heads, N, N)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = attn @ v  # (B, heads, N, head_dim)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.0, attn_drop=0.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=2,\n",
    "                 embed_dim=256, depth=6, num_heads=8, mlp_ratio=4.0,\n",
    "                 attn_drop=0.0, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # [CLS] token & positional embeddings (1-D learnable)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Transformer encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, attn_drop, drop=drop_rate)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "\n",
    "        # Classification head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Parameter init\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)  # (B, N, D)\n",
    "\n",
    "        # prepend cls token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, D)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (B, 1+N, D)\n",
    "\n",
    "        # add pos embeddings\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # transformer\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        cls = x[:, 0]  # [CLS]\n",
    "        logits = self.head(cls)\n",
    "        return logits\n",
    "\n",
    "# Build model (or optionally try a local pretrained ViT via timm/torchvision)\n",
    "def create_model(cfg: Config, num_classes: int):\n",
    "    if cfg.try_pretrained_vit and '_HAVE_TIMM' in globals() and _HAVE_TIMM:\n",
    "        try:\n",
    "            print(\"Trying local pretrained ViT via timm...\")\n",
    "            model = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=num_classes)\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(\"timm pretrained model not available locally or weights download failed:\", e)\n",
    "            print(\"Falling back to from-scratch ViT.\")\n",
    "    model = VisionTransformer(\n",
    "        img_size=cfg.image_size, patch_size=cfg.patch_size, in_chans=cfg.in_channels,\n",
    "        num_classes=num_classes, embed_dim=cfg.embed_dim, depth=cfg.depth, num_heads=cfg.num_heads,\n",
    "        mlp_ratio=cfg.mlp_ratio, attn_drop=cfg.attn_dropout, drop_rate=cfg.drop_rate\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model(cfg, num_classes).to(DEVICE)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(model.__class__.__name__, \"params:\", f\"{n_params/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Training Utilities ====\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        if self.smoothing <= 0.0:\n",
    "            return F.cross_entropy(logits, target)\n",
    "        n_classes = logits.size(-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.fill_(self.smoothing / (n_classes - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))\n",
    "\n",
    "def cosine_scheduler(optimizer, warmup_epochs, total_epochs, base_lr, train_loader_len):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return float(epoch + 1) / float(max(1, warmup_epochs))\n",
    "        progress = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_targets = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, targets in data_loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        logits = model(images)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "        all_logits.append(logits.detach().cpu())\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    probs = all_logits.softmax(dim=1)[:, 1].numpy()\n",
    "    y_true = all_targets.numpy()\n",
    "    acc = correct / total\n",
    "    # F1\n",
    "    if 'sklearn' in sys.modules:\n",
    "        from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_fscore_support\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, np.argmax(all_logits.numpy(), axis=1), average=\"binary\", zero_division=0)\n",
    "        try:\n",
    "            auroc = roc_auc_score(y_true, probs)\n",
    "        except Exception:\n",
    "            auroc = float(\"nan\")\n",
    "    else:\n",
    "        preds = np.argmax(all_logits.numpy(), axis=1)\n",
    "        tp = np.sum((preds == 1) & (y_true == 1))\n",
    "        fp = np.sum((preds == 1) & (y_true == 0))\n",
    "        fn = np.sum((preds == 0) & (y_true == 1))\n",
    "        prec = tp / (tp + fp + 1e-9)\n",
    "        rec = tp / (tp + fn + 1e-9)\n",
    "        f1 = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "        auroc = float(\"nan\")\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"auroc\": float(auroc),\n",
    "        \"logits\": all_logits.numpy(),\n",
    "        \"targets\": y_true,\n",
    "    }\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, scaler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler is not None and device.type == \"cuda\":\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(images)\n",
    "                loss = criterion(logits, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        n += images.size(0)\n",
    "\n",
    "    return running_loss / max(1, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6160c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Train Loop with Early Stopping ====\n",
    "best_val_metric = -float(\"inf\")\n",
    "best_state_dict = None\n",
    "history = {\"train_loss\": [], \"val_acc\": [], \"val_f1\": [], \"val_auroc\": []}\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=cfg.label_smoothing).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "scheduler = cosine_scheduler(optimizer, cfg.warmup_epochs, cfg.epochs, cfg.lr, train_loader_len=len(train_loader))\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "epochs_no_improve = 0\n",
    "for epoch in range(cfg.epochs):\n",
    "    t0 = time.time()\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE, scaler)\n",
    "    scheduler.step()\n",
    "\n",
    "    val_metrics = evaluate(model, val_loader, DEVICE)\n",
    "    val_metric = val_metrics[\"auroc\"] if cfg.early_stop_metric == \"auroc\" else val_metrics[\"f1\"]\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_acc\"].append(val_metrics[\"acc\"])\n",
    "    history[\"val_f1\"].append(val_metrics[\"f1\"])\n",
    "    history[\"val_auroc\"].append(val_metrics[\"auroc\"])\n",
    "\n",
    "    took = time.time() - t0\n",
    "    print(f\"Epoch {epoch+1:02d}/{cfg.epochs} - \"\n",
    "          f\"loss: {train_loss:.4f} | \"\n",
    "          f\"val_acc: {val_metrics['acc']:.4f} | \"\n",
    "          f\"val_f1: {val_metrics['f1']:.4f} | \"\n",
    "          f\"val_auroc: {val_metrics['auroc']:.4f} | \"\n",
    "          f\"time: {took:.1f}s\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    improved = val_metric > best_val_metric\n",
    "    if improved:\n",
    "        best_val_metric = val_metric\n",
    "        best_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= cfg.early_stop_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best {cfg.early_stop_metric}: {best_val_metric:.4f}\")\n",
    "            break\n",
    "\n",
    "# Save best checkpoint\n",
    "OUT_DIR = Path(\"./outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "best_ckpt_path = OUT_DIR / \"vit_chestxray_best.pth\"\n",
    "if best_state_dict is not None:\n",
    "    torch.save({\"state_dict\": best_state_dict, \"config\": asdict(cfg), \"classes\": idx_to_class}, best_ckpt_path)\n",
    "    print(\"Saved best checkpoint to\", best_ckpt_path.resolve())\n",
    "else:\n",
    "    print(\"No improvement recorded; saving last model state.\")\n",
    "    torch.save({\"state_dict\": {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "                \"config\": asdict(cfg), \"classes\": idx_to_class}, best_ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Training Curves ====\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.plot(history[\"val_f1\"], label=\"val_f1\")\n",
    "plt.plot(history[\"val_auroc\"], label=\"val_auroc\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.title(\"Validation Metrics\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38901259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Final Evaluation on UNTOUCHED Test Set ====\n",
    "# Load best checkpoint and evaluate once on test set.\n",
    "ckpt = torch.load(best_ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "test_metrics = evaluate(model, test_loader, DEVICE)\n",
    "print({k: float(v) if isinstance(v, (np.floating,)) else v for k,v in test_metrics.items() if k in [\"acc\", \"precision\", \"recall\", \"f1\", \"auroc\"]})\n",
    "\n",
    "# Classification report & confusion matrix\n",
    "if 'sklearn' in sys.modules:\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "    y_true = test_metrics[\"targets\"]\n",
    "    y_pred = np.argmax(test_metrics[\"logits\"], axis=1)\n",
    "    print(\"\\nClassification report (test):\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[idx_to_class[0], idx_to_class[1]], digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion matrix (test):\\n\", cm)\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(\"Confusion Matrix (Test)\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(idx_to_class))\n",
    "    plt.xticks(tick_marks, [idx_to_class[i] for i in range(len(idx_to_class))], rotation=45)\n",
    "    plt.yticks(tick_marks, [idx_to_class[i] for i in range(len(idx_to_class))])\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    from sklearn.metrics import roc_curve\n",
    "    probs = (test_metrics[\"logits\"])\n",
    "    if hasattr(probs, \"softmax\"):\n",
    "        probs = probs.softmax(axis=1)[:,1]\n",
    "    else:\n",
    "        probs = torch.tensor(probs).softmax(dim=1)[:,1].numpy()\n",
    "    fpr, tpr, _ = roc_curve(y_true, probs)\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    plt.plot(fpr, tpr, linewidth=2)\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC (AUROC={test_metrics['auroc']:.4f})\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Qualitative: a few test predictions ====\n",
    "model.eval()\n",
    "images_shown = 12\n",
    "images, targets = next(iter(test_loader))\n",
    "images = images[:images_shown].to(DEVICE)\n",
    "targets = targets[:images_shown].to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(images)\n",
    "    probs = logits.softmax(dim=1)\n",
    "    preds = probs.argmax(dim=1)\n",
    "\n",
    "# Denormalize for display\n",
    "def denorm(img):\n",
    "    img = img.clone().detach().cpu()\n",
    "    if cfg.in_channels == 3:\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "        std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    else:\n",
    "        mean = torch.tensor([0.5]).view(1,1,1)\n",
    "        std  = torch.tensor([0.25]).view(1,1,1)\n",
    "    img = img * std + mean\n",
    "    return img.clamp(0,1)\n",
    "\n",
    "grid = make_grid(denorm(images), nrow=4)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(np.transpose(grid.numpy(), (1,2,0)))\n",
    "plt.axis(\"off\")\n",
    "title_lines = []\n",
    "for i in range(len(images)):\n",
    "    title_lines.append(f\"{i+1}) pred={idx_to_class[int(preds[i].item())]} ({float(probs[i, preds[i]].item()):.2f}), true={idx_to_class[int(targets[i].item())]}\")\n",
    "plt.title(\"\\n\".join(title_lines), fontsize=9)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Save brief results artifact ====\n",
    "results_path = OUT_DIR / \"test_metrics.json\"\n",
    "import json, numpy as np\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump({k: (float(v) if isinstance(v, (np.floating,)) else v) for k,v in test_metrics.items() if k in [\"acc\",\"precision\",\"recall\",\"f1\",\"auroc\"]}, f, indent=2)\n",
    "print(\"Saved test metrics to\", results_path.resolve())\n",
    "\n",
    "print(\"NOTE: Test set was never used during training or model selection. Validation guided early stopping; test was evaluated once at the end.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e193e18",
   "metadata": {},
   "source": [
    "### Appendix: Using 1‑channel input (optional)\n",
    "\n",
    "By default, we replicate grayscale X‑rays to 3 channels to keep normalization and pretrained compatibility straightforward.\n",
    "If you prefer to keep images as **single‑channel (C=1)**, set `cfg.in_channels = 1` **before** building the model and transforms.\n",
    "The `PatchEmbed` will adapt to `in_chans=1`. Adjust normalization mean/std accordingly in `build_transforms`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
